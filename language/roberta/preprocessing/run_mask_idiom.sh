python tokenize_mask_for_idiom.py \
  --tokenizer_path=/data1/nlp/models/chinese-roberta-wwm-ext-large \
  --seq_len=128 \
  --input_path /data1/yutian.rong/projects/ColossalAI-Examples/data \
  --output_path /data1/yutian.rong/projects/ColossalAI-Examples/data